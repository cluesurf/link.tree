
load @wavebond/snow/amazonaws.com/firehose/2015-08-04/base/block-size-bytes
  take form block-size-bytes

load @wavebond/snow/amazonaws.com/firehose/2015-08-04/base/boolean-object
  take form boolean-object

load @wavebond/snow/amazonaws.com/firehose/2015-08-04/base/non-negative-integer-object
  take form non-negative-integer-object

load @wavebond/snow/amazonaws.com/firehose/2015-08-04/base/parquet-compression
  take form parquet-compression

load @wavebond/snow/amazonaws.com/firehose/2015-08-04/base/parquet-page-size-bytes
  take form parquet-page-size-bytes

load @wavebond/snow/amazonaws.com/firehose/2015-08-04/base/parquet-writer-version
  take form parquet-writer-version

form parquet-ser-de, name <ParquetSerDe>
  note <A serializer to use for converting data to the Parquet format before storing it in Amazon S3. For more information, see [Apache Parquet](https://parquet.apache.org/documentation/latest/).>
  take block-size-bytes, name <BlockSizeBytes>
    like block-size-bytes
    void take
    note <The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.>
  take page-size-bytes, name <PageSizeBytes>
    like parquet-page-size-bytes
    void take
    note <The Parquet page size. Column chunks are divided into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and the default is 1 MiB.>
  take compression, name <Compression>
    like parquet-compression
    void take
    note <The compression code to use over data blocks. The possible values are `UNCOMPRESSED`, `SNAPPY`, and `GZIP`, with the default being `SNAPPY`. Use `SNAPPY` for higher decompression speed. Use `GZIP` if the compression ratio is more important than speed.>
  take enable-dictionary-compression, name <EnableDictionaryCompression>
    like boolean-object
    void take
    note <Indicates whether to enable dictionary compression.>
  take max-padding-bytes, name <MaxPaddingBytes>
    like non-negative-integer-object
    void take
    note <The maximum amount of padding to apply. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 0.>
  take writer-version, name <WriterVersion>
    like parquet-writer-version
    void take
    note <Indicates the version of row format to output. The possible values are `V1` and `V2`. The default is `V1`.>