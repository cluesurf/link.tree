
load @termsurf/snow/googleapis.com/container/v1beta1/base/max-pods-constraint
  take form max-pods-constraint

load @termsurf/snow/googleapis.com/container/v1beta1/base/node-config
  take form node-config

load @termsurf/snow/googleapis.com/container/v1beta1/base/node-management
  take form node-management

load @termsurf/snow/googleapis.com/container/v1beta1/base/node-network-config
  take form node-network-config

load @termsurf/snow/googleapis.com/container/v1beta1/base/node-pool-autoscaling
  take form node-pool-autoscaling

load @termsurf/snow/googleapis.com/container/v1beta1/base/placement-policy
  take form placement-policy

load @termsurf/snow/googleapis.com/container/v1beta1/base/upgrade-settings
  take form upgrade-settings

form node-pool, name <NodePool>
  note <NodePool contains the name and configuration for a cluster's node pool. Node pools are a set of nodes (i.e. VM's), with a common configuration and specification, under the control of the cluster master. They may have a set of Kubernetes labels applied to them, which may be used to reference them during pod scheduling. They may also be resized up or down, to accommodate the workload. These upgrade settings control the level of parallelism and the level of disruption caused by an upgrade. maxUnavailable controls the number of nodes that can be simultaneously unavailable. maxSurge controls the number of additional nodes that can be added to the node pool temporarily for the time of the upgrade to increase the number of available nodes. (maxUnavailable + maxSurge) determines the level of parallelism (how many nodes are being upgraded at the same time). Note: upgrades inevitably introduce some disruption since workloads need to be moved from old nodes to new, upgraded ones. Even if maxUnavailable=0, this holds true. (Disruption stays within the limits of PodDisruptionBudget, if it is configured.) Consider a hypothetical node pool with 5 nodes having maxSurge=2, maxUnavailable=1. This means the upgrade process upgrades 3 nodes simultaneously. It creates 2 additional (upgraded) nodes, then it brings down 3 old (not yet upgraded) nodes at the same time. This ensures that there are always at least 4 nodes available.>
  take autoscaling, name <autoscaling>
    like node-pool-autoscaling
    void take
  take conditions, name <conditions>
    void take
  take config, name <config>
    like node-config
    void take
  take initial-node-count, name <initialNodeCount>
    void take
  take instance-group-urls, name <instanceGroupUrls>
    void take
  take locations, name <locations>
    void take
  take management, name <management>
    like node-management
    void take
  take max-pods-constraint, name <maxPodsConstraint>
    like max-pods-constraint
    void take
  take name, name <name>
    void take
  take network-config, name <networkConfig>
    like node-network-config
    void take
  take placement-policy, name <placementPolicy>
    like placement-policy
    void take
  take pod-ipv4-cidr-size, name <podIpv4CidrSize>
    void take
  take self-link, name <selfLink>
    void take
  take status, name <status>
    void take
  take status-message, name <statusMessage>
    void take
  take upgrade-settings, name <upgradeSettings>
    like upgrade-settings
    void take
  take version, name <version>
    void take